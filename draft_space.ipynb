{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3051a570-cabf-4dde-8e14-78751526ccd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-16 23:56:31.692565: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-16 23:56:31.711393: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-16 23:56:31.711409: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-16 23:56:31.711427: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-16 23:56:31.715577: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-16 23:56:32.102320: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-12-16 23:56:32.907380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-16 23:56:32.965073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-16 23:56:32.965208: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-16 23:56:32.966831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-16 23:56:32.966946: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-16 23:56:32.967037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-16 23:56:33.011771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-16 23:56:33.011887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-16 23:56:33.011986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-16 23:56:33.012064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21645 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VGG16...\n",
      "Found 0 images belonging to 12 classes.\n",
      "Found 0 images belonging to 12 classes.\n",
      "Found 0 images belonging to 12 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected value for `steps_per_epoch`. Received value is 0. Please check the docstring for `model.fit()` for supported values.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 79\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# VGG16\u001b[39;00m\n\u001b[1;32m     78\u001b[0m vgg16_base_model \u001b[38;5;241m=\u001b[39m VGG16(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m, include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m---> 79\u001b[0m vgg16_model, vgg16_test_generator, vgg16_history \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_and_train_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvgg16_base_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVGG16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# ResNet50\u001b[39;00m\n\u001b[1;32m     82\u001b[0m resnet50_base_model \u001b[38;5;241m=\u001b[39m ResNet50(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m, include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n",
      "Cell \u001b[0;32mIn[1], line 54\u001b[0m, in \u001b[0;36mbuild_and_train_model\u001b[0;34m(base_model, model_name, train_dir, validation_dir, test_dir, input_shape)\u001b[0m\n\u001b[1;32m     51\u001b[0m validation_generator \u001b[38;5;241m=\u001b[39m create_data_generator(validation_dir, ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m), input_shape)\n\u001b[1;32m     52\u001b[0m test_generator \u001b[38;5;241m=\u001b[39m create_data_generator(test_dir, ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m), input_shape)\n\u001b[0;32m---> 54\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Evaluate on test data\u001b[39;00m\n\u001b[1;32m     63\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_generator, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_generator))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1275\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m steps_per_epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected value for `steps_per_epoch`. Received value is 0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1277\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the docstring for `model.fit()` for supported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1279\u001b[0m     )\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_epoch \u001b[38;5;241m=\u001b[39m steps_per_epoch\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;66;03m# `steps_per_execution_value` is the cached initial value.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;66;03m# `steps_per_execution` is mutable and may be changed by the DataAdapter\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m# to handle partial executions.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected value for `steps_per_epoch`. Received value is 0. Please check the docstring for `model.fit()` for supported values."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, InceptionV3, MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define your data directories\n",
    "train_dir = 'COP_banknote/ds3/Train'\n",
    "test_dir = 'COP_banknote/ds3/Test'\n",
    "validation_dir = 'COP_banknote/ds3/Validation'\n",
    "\n",
    "# Common Hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "num_classes = 12\n",
    "\n",
    "# Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Define a function to build and train a model\n",
    "def build_and_train_model(base_model, model_name, train_dir, validation_dir, test_dir, input_shape):\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Freeze the layers from the base model\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(f\"Training {model_name}...\")\n",
    "\n",
    "    train_generator = create_data_generator(train_dir, train_datagen, input_shape)\n",
    "    validation_generator = create_data_generator(validation_dir, ImageDataGenerator(rescale=1./255), input_shape)\n",
    "    test_generator = create_data_generator(test_dir, ImageDataGenerator(rescale=1./255), input_shape)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        epochs=num_epochs\n",
    "    )\n",
    "\n",
    "    # Evaluate on test data\n",
    "    test_loss, test_acc = model.evaluate(test_generator, steps=len(test_generator))\n",
    "    print(f\"{model_name} Test accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "    return model, test_generator, history\n",
    "\n",
    "def create_data_generator(data_dir, data_generator, input_shape):\n",
    "    return data_generator.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=input_shape,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        classes=['100k', '100kf', '10k', '10kf', '50k', '50kf', '5k', '5kf', '20k', '20kf', '2k', '2kf']\n",
    "    )\n",
    "\n",
    "# VGG16\n",
    "vgg16_base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "vgg16_model, vgg16_test_generator, vgg16_history = build_and_train_model(vgg16_base_model, \"VGG16\", train_dir, validation_dir, test_dir, (224, 224))\n",
    "\n",
    "# ResNet50\n",
    "resnet50_base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "resnet50_model, resnet50_test_generator, resnet50_history = build_and_train_model(resnet50_base_model, \"ResNet50\", train_dir, validation_dir, test_dir, (224, 224))\n",
    "\n",
    "# InceptionV3\n",
    "inceptionv3_base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "inceptionv3_model, inceptionv3_test_generator, inceptionv3_history = build_and_train_model(inceptionv3_base_model, \"InceptionV3\", train_dir, validation_dir, test_dir, (299, 299))\n",
    "\n",
    "# MobileNetV2\n",
    "mobilenetv2_base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "mobilenetv2_model, mobilenetv2_test_generator, mobilenetv2_history = build_and_train_model(mobilenetv2_base_model, \"MobileNetV2\", train_dir, validation_dir, test_dir, (224, 224))\n",
    "\n",
    "# Calculate metrics for each model\n",
    "def calculate_metrics(model, model_name, test_generator):\n",
    "    y_test = test_generator.classes\n",
    "    Y_pred = model.predict(test_generator)\n",
    "    Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, Y_pred_classes)\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, Y_pred_classes, target_names=test_generator.class_indices.keys())\n",
    "\n",
    "    print(f\"Metrics for {model_name}:\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "\n",
    "# Calculate metrics for each model\n",
    "calculate_metrics(vgg16_model, \"VGG16\", vgg16_test_generator)\n",
    "calculate_metrics(resnet50_model, \"ResNet50\", resnet50_test_generator)\n",
    "calculate_metrics(inceptionv3_model, \"InceptionV3\", inceptionv3_test_generator)\n",
    "calculate_metrics(mobilenetv2_model, \"MobileNetV2\", mobilenetv2_test_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e144c2-6a96-44a2-9bf6-aa8243c50c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137cf93c-82c3-442f-82fd-254d75b2f35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89037fd4-4833-4418-9060-320afb655cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionv3_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62719b98-fb58-4fc7-b38a-4a67c542243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenetv2_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7154e13-9cb7-495e-9085-de22d306227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_training_metrics(history, model_name):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{model_name} Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'{model_name} Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "plot_training_metrics(vgg16_history, \"VGG16\")\n",
    "plot_training_metrics(resnet50_history, \"ResNet50\")\n",
    "plot_training_metrics(inceptionv3_history, \"InceptionV3\")\n",
    "plot_training_metrics(mobilenetv2_history, \"MobileNetV2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdec95f-312d-4ec3-a072-b49a04c67996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to plot micro-average ROC curve for a model\n",
    "def plot_micro_roc_curve(model, model_name, test_generator):\n",
    "    y_test = test_generator.classes\n",
    "    Y_pred = model.predict(test_generator)\n",
    "\n",
    "    # Binarize the labels\n",
    "    lb = LabelBinarizer()\n",
    "    y_test_bin = lb.fit_transform(y_test)\n",
    "\n",
    "    # Calculate micro-average ROC and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin.ravel(), Y_pred.ravel())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Micro-average ROC Curve for {model_name}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "# Plot micro-average ROC curve for each model\n",
    "plot_micro_roc_curve(vgg16_model, \"VGG16\", vgg16_test_generator)\n",
    "plot_micro_roc_curve(resnet50_model, \"ResNet50\", resnet50_test_generator)\n",
    "plot_micro_roc_curve(inceptionv3_model, \"InceptionV3\", inceptionv3_test_generator)\n",
    "plot_micro_roc_curve(mobilenetv2_model, \"MobileNetV2\", mobilenetv2_test_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4070a-2ada-4475-99c1-20e09a5a44e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "def calculate_mAP(model, test_generator, model_name):\n",
    "    y_true = test_generator.classes\n",
    "    Y_pred = model.predict(test_generator)\n",
    "    \n",
    "    # Calculate the Average Precision (AP) for each class\n",
    "    APs = []\n",
    "    for class_idx in range(num_classes):\n",
    "        y_true_class = (y_true == class_idx)\n",
    "        y_pred_class = Y_pred[:, class_idx]\n",
    "        ap = average_precision_score(y_true_class, y_pred_class)\n",
    "        APs.append(ap)\n",
    "    \n",
    "    # Calculate the Mean Average Precision (mAP)\n",
    "    mAP = np.mean(APs)\n",
    "    \n",
    "    print(f\"Mean Average Precision (mAP) for {model_name}: {mAP:.4f}\")\n",
    "\n",
    "# Calculate mAP for each model\n",
    "calculate_mAP(vgg16_model, vgg16_test_generator, \"VGG16\")\n",
    "calculate_mAP(resnet50_model, resnet50_test_generator, \"ResNet50\")\n",
    "calculate_mAP(inceptionv3_model, inceptionv3_test_generator, \"InceptionV3\")\n",
    "calculate_mAP(mobilenetv2_model, mobilenetv2_test_generator, \"MobileNetV2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84b3b74-c073-4ed5-81cf-4cc6285f4aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_predictions(model, model_name, test_generator, num_samples=5):\n",
    "    # Get a batch of test data and their true labels\n",
    "    test_images, true_labels = next(test_generator)\n",
    "\n",
    "    # Get model predictions for the batch\n",
    "    predicted_labels = model.predict(test_images)\n",
    "\n",
    "    # Get class labels and indices\n",
    "    class_labels = test_generator.class_indices\n",
    "    class_indices = {v: k for k, v in class_labels.items()}\n",
    "\n",
    "    # Plot the images and their predictions\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(test_images[i])\n",
    "        true_label = class_indices[np.argmax(true_labels[i])]\n",
    "        predicted_label = class_indices[np.argmax(predicted_labels[i])]\n",
    "        plt.title(f'True: {true_label}\\nPredicted: {predicted_label}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.suptitle(f'{model_name} - Visual Inspection')\n",
    "    plt.show()\n",
    "\n",
    "# Visual inspection for VGG16\n",
    "visualize_predictions(vgg16_model, \"VGG16\", vgg16_test_generator)\n",
    "\n",
    "# Visual inspection for ResNet50\n",
    "visualize_predictions(resnet50_model, \"ResNet50\", resnet50_test_generator)\n",
    "\n",
    "# Visual inspection for InceptionV3\n",
    "visualize_predictions(inceptionv3_model, \"InceptionV3\", inceptionv3_test_generator)\n",
    "\n",
    "# Visual inspection for MobileNetV2\n",
    "visualize_predictions(mobilenetv2_model, \"MobileNetV2\", mobilenetv2_test_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c2495a-0f6c-4f8a-a1c0-d583fa176d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature maps\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load the image\n",
    "image_path = '914.jpg'\n",
    "img = image.load_img(image_path, target_size=(224, 224))\n",
    "img = image.img_to_array(img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "img = preprocess_input(img)\n",
    "\n",
    "# Define a function to visualize feature maps\n",
    "def visualize_feature_maps(model, img, layer_names, max_images=3):\n",
    "    layer_outputs = [layer.output for layer in model.layers if layer.name in layer_names]\n",
    "    activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "    intermediate_activations = activation_model.predict(img)\n",
    "    \n",
    "    for layer_name, layer_activation in zip(layer_names, intermediate_activations):\n",
    "        n_features = layer_activation.shape[-1]\n",
    "        n_features = min(n_features, max_images)\n",
    "        size = layer_activation.shape[1]\n",
    "        n_cols = n_features // max_images\n",
    "        display_grid = np.zeros((size * n_cols, max_images * size))\n",
    "        \n",
    "        for col in range(n_cols):\n",
    "            for row in range(max_images):\n",
    "                channel_index = col * max_images + row\n",
    "                if channel_index < n_features:\n",
    "                    channel_image = layer_activation[0, :, :, channel_index]\n",
    "                    channel_image -= channel_image.mean()\n",
    "                    channel_image /= (channel_image.std() + 1e-5)\n",
    "                    channel_image *= 64\n",
    "                    channel_image += 128\n",
    "                    channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "                    display_grid[col * size : (col + 1) * size, row * size : (row + 1) * size] = channel_image\n",
    "        \n",
    "        scale = 1. / size\n",
    "        plt.figure(figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0]))\n",
    "        plt.axis('off')\n",
    "        plt.title(f'{layer_name} Feature Maps')\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "        plt.show()\n",
    "\n",
    "# Define a function to visualize ConvNet filters\n",
    "def visualize_convnet_filters(model, layer_names, max_filters=3):\n",
    "    for layer_name in layer_names:\n",
    "        layer = model.get_layer(layer_name)\n",
    "        if layer and len(layer.get_weights()) > 0:\n",
    "            weights = layer.get_weights()[0]\n",
    "            filter_count = min(weights.shape[3], max_filters)\n",
    "        \n",
    "            plt.figure(figsize=(10, 5))\n",
    "            for i in range(filter_count):\n",
    "                plt.subplot(1, filter_count, i+1)\n",
    "                plt.imshow(weights[:, :, 0, i], cmap='viridis')\n",
    "                plt.axis('off')\n",
    "                plt.title(f'{layer_name} Filter {i+1}')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"No weights found for layer: {layer_name}\")\n",
    "\n",
    "# Define a function to visualize class output (prediction)\n",
    "def visualize_class_output(model, img, top_k=3):\n",
    "    predictions = model.predict(img)\n",
    "    decoded_predictions = decode_predictions(predictions, top=top_k)[0]\n",
    "    \n",
    "    labels = [label for (_, label, prob) in decoded_predictions]\n",
    "    probabilities = [prob for (_, label, prob) in decoded_predictions]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.barh(labels, probabilities)\n",
    "    plt.xlabel('Probability')\n",
    "    plt.title('Top Predicted Classes')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "# Load models\n",
    "vgg16_model = VGG16(weights='imagenet', include_top=True)\n",
    "resnet50_model = ResNet50(weights='imagenet', include_top=True)\n",
    "inceptionv3_model = InceptionV3(weights='imagenet', include_top=True)\n",
    "mobilenetv2_model = MobileNetV2(weights='imagenet', include_top=True)\n",
    "\n",
    "# Define layer names to visualize\n",
    "vgg16_layer_names = ['block1_conv2', 'block2_conv2', 'block3_conv3']\n",
    "resnet50_layer_names = ['conv1_relu', 'conv2_block3_3_conv', 'conv3_block4_6_conv']\n",
    "inceptionv3_layer_names = ['mixed0', 'mixed1', 'mixed2']\n",
    "mobilenetv2_layer_names = ['block_1_project_BN', 'block_3_project_BN', 'block_16_project_BN']\n",
    "\n",
    "# Visualize feature maps, ConvNet filters, and class output for each model\n",
    "visualize_feature_maps(vgg16_model, img, vgg16_layer_names)\n",
    "visualize_convnet_filters(vgg16_model, vgg16_layer_names)\n",
    "visualize_class_output(vgg16_model, img)\n",
    "\n",
    "\n",
    "\"\"\"this will be used only if VGG16 is not the best model\n",
    "\n",
    "\n",
    "visualize_feature_maps(inceptionv3_model, img, inceptionv3_layer_names)\n",
    "visualize_convnet_filters(inceptionv3_model, inceptionv3_layer)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bb7803-dec6-4165-bbbb-5411e93bae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_feature_maps(resnet50_model, img, resnet50_layer_names)\n",
    "\n",
    "visualize_class_output(resnet50_model, img)\n",
    "visualize_convnet_filters(resnet50_model, resnet50_layer_names)\n",
    "\n",
    "\"convnet_filters not working----- visualize_convnet_filters(resnet50_model, resnet50_layer_names)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57824cc-6b7d-4290-85e5-ee194bc0da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_feature_maps(inceptionv3_model, img, inceptionv3_layer_names)\n",
    "visualize_convnet_filters(inceptionv3_model, inceptionv3_layer)\n",
    "visualize_class_output(inceptionv3_model, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fc3fcb-42b1-453f-aef9-c22dc9827236",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_feature_maps(vgg16_model, img, vgg16_layer_names)\n",
    "visualize_convnet_filters(vgg16_model, vgg16_layer_names)\n",
    "visualize_class_output(vgg16_model, img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
